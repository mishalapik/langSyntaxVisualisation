{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mixai\\python projects\\Visualisation\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mixai\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk \n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import stanza\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-25 19:09:19 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json: 379kB [00:00, 4.03MB/s]                    \n",
      "2024-03-25 19:09:19 INFO: Downloaded file to C:\\Users\\mixai\\stanza_resources\\resources.json\n",
      "Downloading https://huggingface.co/stanfordnlp/stanza-ru/resolve/v1.8.0/models/pos/syntagrus_charlm.pt: 100%|██████████| 38.8M/38.8M [00:16<00:00, 2.32MB/s]\n",
      "2024-03-25 19:09:40 INFO: Loading these models for language: ru (Russian):\n",
      "==================================\n",
      "| Processor | Package            |\n",
      "----------------------------------\n",
      "| tokenize  | syntagrus          |\n",
      "| pos       | syntagrus_charlm   |\n",
      "| lemma     | syntagrus_nocharlm |\n",
      "| depparse  | syntagrus_charlm   |\n",
      "| ner       | wikiner            |\n",
      "==================================\n",
      "\n",
      "2024-03-25 19:09:40 INFO: Using device: cpu\n",
      "2024-03-25 19:09:40 INFO: Loading: tokenize\n",
      "2024-03-25 19:09:41 INFO: Loading: pos\n",
      "2024-03-25 19:09:41 INFO: Loading: lemma\n",
      "2024-03-25 19:09:42 INFO: Loading: depparse\n",
      "2024-03-25 19:09:42 INFO: Loading: ner\n",
      "2024-03-25 19:09:43 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_excel('fill_info.xlsx')\n",
    "df_ml = df\n",
    "\n",
    "full_corpus = df_ml[\"TEXT\"].values\n",
    "\n",
    "sentences = [sent for corp in full_corpus for sent in sent_tokenize(corp, language=\"russian\")]\n",
    "\n",
    "long_sents = [i for i in sentences if len(i) > 30]\n",
    "\n",
    "nlp = stanza.Pipeline(lang='ru',\n",
    "processors='tokenize,pos,lemma,ner,depparse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 526/526 [08:36<00:00,  1.02it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "triplets = []\n",
    "for s in tqdm(long_sents):\n",
    "    doc = nlp(s)\n",
    "    for sent in doc.sentences:\n",
    "        entities = [ent.text for ent in sent.ents]\n",
    "        res_d = dict()\n",
    "        temp_d = dict()\n",
    "        for word in sent.words:\n",
    "            temp_d[word.text] = {\"head\": sent.words[word.head-1].text, \"dep\": word.deprel, \"id\": word.id}\n",
    "        for k in temp_d.keys():\n",
    "            nmod_1 = \"\"\n",
    "            nmod_2 = \"\"\n",
    "            if (temp_d[k][\"dep\"] in [\"nsubj\", \"nsubj:pass\"]) & (k in entities):\n",
    "                res_d[k] = {\"head\": temp_d[k][\"head\"]}\n",
    "                \n",
    "                for k_0 in temp_d.keys():\n",
    "                    if (temp_d[k_0][\"dep\"] in [\"obj\", \"obl\"]) &\\\n",
    "                       (temp_d[k_0][\"head\"] == res_d[k][\"head\"]) &\\\n",
    "                        (temp_d[k_0][\"id\"] > temp_d[res_d[k][\"head\"]][\"id\"]):\n",
    "                        res_d[k][\"obj\"] = k_0\n",
    "                        break\n",
    "\n",
    "                for k_1 in temp_d.keys():\n",
    "                    if (temp_d[k_1][\"head\"] == res_d[k][\"head\"]) & (k_1 == \"не\"):\n",
    "                        res_d[k][\"head\"] = \"не \"+res_d[k][\"head\"]\n",
    "                \n",
    "                if \"obj\" in res_d[k].keys():\n",
    "                    for k_4 in temp_d.keys():\n",
    "                        if (temp_d[k_4][\"dep\"] ==\"nmod\") &\\\n",
    "                           (temp_d[k_4][\"head\"] == res_d[k][\"obj\"]):\n",
    "                            nmod_1 = k_4\n",
    "                            break\n",
    "                            \n",
    "                    for k_5 in temp_d.keys():\n",
    "                        if (temp_d[k_5][\"dep\"] ==\"nummod\") &\\\n",
    "                           (temp_d[k_5][\"head\"] == nmod_1):\n",
    "                            nmod_2 = k_5\n",
    "                            break\n",
    "                    res_d[k][\"obj\"] = res_d[k][\"obj\"]+\" \"+nmod_2+\" \"+nmod_1\n",
    "\n",
    "        if len(res_d) > 0:\n",
    "            triplets.append([s, res_d])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = set()\n",
    "clear_triplets = []\n",
    "for tr in triplets:\n",
    "    for k in tr[1].keys():\n",
    "        if \"obj\" in tr[1][k].keys():\n",
    "            clear_triplets.append([tr[0], k, tr[1][k]['head'], tr[1][k]['obj']])\n",
    "            nodes.add(k)\n",
    "            nodes.add(tr[1][k]['obj'])\n",
    "\n",
    "\n",
    "word_num = dict()\n",
    "for c, word in enumerate(nodes):\n",
    "    word_num[word] = c+1\n",
    "\n",
    "import script_for_graph\n",
    "import importlib\n",
    "importlib.reload(script_for_graph)\n",
    "from script_for_graph import header_text, tail_text\n",
    "\n",
    "header_text += \"\"\"\\nvar nodes = new vis.DataSet([\\n\"\"\"\n",
    "for w in nodes:\n",
    "    header_text += \"{\"\n",
    "    header_text += f\"\"\"         id: {word_num[w]},\n",
    "                                label: \"{w}\"\\n\"\"\"\n",
    "    header_text += \"},\"\n",
    "header_text += \"   ]);\\n\"\n",
    "\n",
    "header_text += \"\"\"var edges = new vis.DataSet([\"\"\"\n",
    "for triplet in clear_triplets:\n",
    "    info = '{\"\":\"<>\"}'.replace(\"<>\",triplet[0])\n",
    "    header_text += \"{\"\n",
    "    header_text += f\"\"\"       from: {word_num[triplet[1]]}, \n",
    "                    to: {word_num[triplet[3]]}, \n",
    "                    arrows: \"to\",\n",
    "                    label: \"{triplet[2]}\",\n",
    "                    info: {info}\\n\"\"\"\n",
    "    header_text +=\"},\"\n",
    "header_text += \"   ]);\\n\"\n",
    "\n",
    "full_text = \"\"\n",
    "full_text += header_text\n",
    "full_text += tail_text\n",
    "\n",
    "with open(f\"page.html\", \"w\", encoding=\"utf-8\") as f: \n",
    "    f.write(full_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
